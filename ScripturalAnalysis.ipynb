{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "# Load Google's pre-trained Word2Vec model.\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unknown words mappings and clean_text function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.tag import pos_tag\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "# nltk.download()\n",
    "\n",
    "TAG_REMOVE = re.compile(r'<[^>]+>')\n",
    "mapping = [('twenti ','twentieth '),('thirti ','thirtieth '),('forti ','fortieth  '),('fifti ','fiftieth '),('sixti ','sixtieth '),('seventi ','seventieth '),('eighti ','eightieth '),('nineti ','ninetieth '),\n",
    "           ('arrogancy ','arrogance '),('tortur ','torture '),('desirest','desire'),('desirest ','desire '),('desir ','desire '),('receivedst ','received '),('carri ','carries '),('forgiv ','forgives '),('divid ','divides '),\n",
    "           ('athirst ','thirsty '),('testifi ','testifies '),('reprove ','reprove '),('grey ','gray '),('aveng ','avenge '),('behoov ','behoove '),('reprov ','reprove '),('affrighted ','frightened '),\n",
    "           ('leathern ','leather '),('abhorr ','abhor '),('rejoic ','rejoice '),('provok ','provoke '),('straiten ','straighten '),('cumbered ','encumbered '),('suffic ','suffice '),('sware ','swore '),\n",
    "           ('justifi ','justify '),('decre ','decree '),('ceas  ','cease '),('mingl ','mingle '),('persuad ','persuade '),('entic ','entice '),('trembl ','tremble '),('cimeter','scimitar'),\n",
    "           ('condemneth','condemn'),('prophesi ','prophesy '),('awak ','awake '),('canst ','cannot '),('engraven','engraved'),('hadst ','had '),('rebell ','rebels '),('persecut ','persecute '),\n",
    "           ('beginn ','begins '),('spilt','spilled'),('deni ','denies '),('consignation ','consignment '),('trampl ','trample '),('unstop ','unseal '),('suppos ','supposes '),('dispis ','despise '),\n",
    "           ('stirr ','stirs '),('harv ','harvest '),('satisfi ','satisfies '),('advocateth ','advocates '),('manif ','manifest '),('hindereth ','hinder '),('forgett ','forget '),('reigneth ','reigns '),\n",
    "           ('fainteth','faints '),('defence ','defense '),('hubl ','hubles '),('shouldst ','should '),('acknowledg ','acknowledges '),('meaneth ','means '),('comfortedst ','comforted '),('griev ','grieves '),\n",
    "           ('methought ','I thought '),('invit ','invites '),('vainness ','vanity '),('shedd ','sheds '),('cleav ','cleaves '),('fulfil ','fulfill '),('choic ','well chosen '),('endur ','endures '),('smit ','smites '),\n",
    "           ('allott ','allots '),('declar ','declares '),('sepulchre ','tomb '),('saidst ','said '),('Believ ','believe '),('Canst ','cannot ')]\n",
    "#put everything to lowercase?\n",
    "\n",
    "def clean_text(text): #removes punctuation, html tags, and pronouns that are not listed above\n",
    "#     clean = TAG_REMOVE.sub('', text)\n",
    "    clean = text.replace('<i>', '')\n",
    "    clean = clean.replace('</i>','')\n",
    "    clean = \" \".join(re.compile('\\w+').findall(clean))\n",
    "    if 'LORD' in clean:\n",
    "        clean = clean.replace('LORD','lord')\n",
    "    clean = \" \".join(clean.split('eth '))\n",
    "    clean = \" \".join(clean.split('est '))\n",
    "    for word in stopwords.words('english'):\n",
    "        if word not in model:\n",
    "            clean = clean.replace(' '+ word +' ', ' ')\n",
    "    for k, v in mapping: #replace words that shouldn't have 'eth' or 'est' taken off\n",
    "        clean = clean.replace(k, v)\n",
    "    return clean\n",
    "\n",
    "# bibles = pd.read_csv( 'bibles.csv' )\n",
    "# bible_file = bibles[[\"Verse\", \"King James Bible\"]]\n",
    "\n",
    "# bible = {}\n",
    "# #Example: bible['Matthew'][1][1] is Matthew 1:1\n",
    "# for i in range(len(bible_file)):\n",
    "#     book_chp_vrs = bible_file[\"Verse\"][i].split()\n",
    "#     book = \" \".join(book_chp_vrs[:-1])\n",
    "#     chap, verse = book_chp_vrs[-1].split(\":\")\n",
    "#     chap = int(chap)\n",
    "#     verse = int(verse)\n",
    "#     text = bible_file[\"King James Bible\"][i]\n",
    "#     if book not in bible.keys():\n",
    "#         if chap == 1 and verse == 1:\n",
    "#             bible[book] = [[],['',clean_text(text)]] #insert clean text\n",
    "#         else:\n",
    "#             print(\"Error, not chapter 1\")\n",
    "#     else:\n",
    "#         if len(bible[book]) == chap:\n",
    "#             bible[book].append([clean_text(text)])\n",
    "#         else:\n",
    "            \n",
    "#             bible[book][chap].append(clean_text(text)) #insert clean text \n",
    "            \n",
    "# # for key in bible.keys():\n",
    "# #     for chapter in bible[key]:\n",
    "# #         for verse in chapter:\n",
    "# #             print(verse)\n",
    "\n",
    "# print(\"bible done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get BoM into dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoM done\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def read_chapter(file, line, BoM):\n",
    "    blank = next(file)\n",
    "    line = next(file)\n",
    "    while True: #Read each verse\n",
    "        if line == 'THE END':\n",
    "            break\n",
    "        \n",
    "        if chap_header_pattern.match(line):\n",
    "            while not book_chp_vrs_pattern.match(line):\n",
    "                line = next(file)\n",
    "        book_chp_vrs = line.split()\n",
    "        book = \" \".join(book_chp_vrs[:-1])\n",
    "        chap, verse = book_chp_vrs[-1].split(\":\")\n",
    "        chap = int(chap)\n",
    "        verse = int(verse)\n",
    "        text = next(file).split()\n",
    "        text = \" \".join(text[1:])\n",
    "        text = text.replace('\\n','') + ' '\n",
    "        line = text\n",
    "        count = 0\n",
    "        while True: #Read full text of verse\n",
    "            count += 1\n",
    "            line = next(file)\n",
    "            if line == '\\n':\n",
    "                while not book_chp_vrs_pattern.match(line) and line != 'THE END':\n",
    "                    line = next(file)\n",
    "                break\n",
    "            text += line.replace('\\n', ' ')\n",
    "            if count > 100:\n",
    "                print(\"100 error\")\n",
    "                break\n",
    "        #Put verse in dicionary\n",
    "        if book not in BoM.keys():\n",
    "            if chap == 1 and verse == 1:\n",
    "                BoM[book] = [[],['',clean_text(text).split()]] #insert clean text\n",
    "            else:\n",
    "                print(\"Error, not chapter 1\")\n",
    "        else:\n",
    "            if len(BoM[book]) == chap:\n",
    "                BoM[book].append([clean_text(text).split()])\n",
    "            else:\n",
    "                BoM[book][chap].append(clean_text(text).split()) #insert clean text\n",
    "            \n",
    "\n",
    "pattern = re.compile(\"(Chapter \\d+)\")\n",
    "book_chp_vrs_pattern = re.compile(\"(\\d*( )*([A-Za-z]+ )*\\d+:\\d+)\")\n",
    "chap_header_pattern = re.compile(\"(\\d*( )*([A-Za-z]+ )*\\d+$)\")\n",
    "\n",
    "bom_file = open('book_of_mormon.txt')\n",
    "BoM = {}\n",
    "for line in bom_file:\n",
    "    if pattern.match(line):\n",
    "#         line, bom_file = read_chapter(bom_file, line)\n",
    "        read_chapter(bom_file, line, BoM)\n",
    "        \n",
    "print(\"BoM done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert BoM to vectors  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "known words done\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "unknown_words = {''}\n",
    "for key in BoM.keys():\n",
    "    for chapter in range(len(BoM[key])):\n",
    "        for verse in BoM[key][chapter]:\n",
    "            for word in verse:\n",
    "                if word not in model:\n",
    "                    unknown_words.add(word)       \n",
    "unknown_words.remove('')\n",
    "\n",
    "# print(unknown_words)\n",
    "# print(len(unknown_words))\n",
    "\n",
    "\n",
    "def average_unknown_word(word_ind, verse):\n",
    "    if word_ind - 2 < 0:\n",
    "        begin = 0\n",
    "    else:\n",
    "        begin = word_ind - 2\n",
    "    if word_ind + 2 > len(verse):\n",
    "        end = len(verse)\n",
    "    else:\n",
    "        end = word_ind + 2\n",
    "    numVerses = 0\n",
    "    vec = np.zeros((300,))\n",
    "    for word in range(begin,end):\n",
    "        if type(verse[word]) != type('str'):\n",
    "            vec += verse[word]\n",
    "            numVerses += 1\n",
    "    if numVerses != 0:\n",
    "        vec /= numVerses\n",
    "    else:\n",
    "        print(\"zero error\")\n",
    "    return vec\n",
    "\n",
    "vec_BoM = copy.deepcopy(BoM)\n",
    "\n",
    "for key in BoM.keys():\n",
    "    for chapter in range(1,len(BoM[key])):\n",
    "        for verse in range(1,len(BoM[key][chapter])):\n",
    "            for word in range(1,len(BoM[key][chapter][verse])):\n",
    "                cur_word = BoM[key][chapter][verse][word]\n",
    "                if cur_word not in unknown_words:\n",
    "                    vec_BoM[key][chapter][verse][word] = model[cur_word]\n",
    "\n",
    "print('known words done')\n",
    "\n",
    "for key in BoM.keys():\n",
    "    for chapter in range(1,len(BoM[key])):\n",
    "        for verse in range(1,len(BoM[key][chapter])):\n",
    "            for word in range(1,len(BoM[key][chapter][verse])):\n",
    "                cur_word = BoM[key][chapter][verse][word]\n",
    "                if cur_word in unknown_words:\n",
    "                    vec_BoM[key][chapter][verse][word] = average_unknown_word(word, vec_BoM[key][chapter][verse])\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "for key in BoM.keys():\n",
    "    for chapter in range(1,len(BoM[key])):\n",
    "        for verse in range(1,len(BoM[key][chapter])):\n",
    "            for word in range(1,len(BoM[key][chapter][verse])):\n",
    "                cur_word = BoM[key][chapter][verse][word]\n",
    "                if type(vec_BoM[key][chapter][verse][word]) == type('str'):\n",
    "                    print(vec_BoM[key][chapter][verse][word])\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bible done\n",
      "len\n",
      "120697\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "from gensim.corpora.wikicorpus import WikiCorpus\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models import word2vec\n",
    "from nltk.corpus import gutenberg\n",
    "from string import punctuation\n",
    "\n",
    "\n",
    "sents = gutenberg.sents('bible-kjv.txt')\n",
    "for file in gutenberg.fileids():\n",
    "    if file != 'bible-kjv.txt':\n",
    "        sents += gutenberg.sents(file)\n",
    "\n",
    "# for sent in sents:\n",
    "#     print(sent)\n",
    "\n",
    "# print(len(bible_kjv_sents))\n",
    "# print(len(sents))\n",
    "\n",
    "discard_punctuation_and_lowercased_sents = [[word.lower() for word in sent if word not in punctuation] for sent in sents]\n",
    "# # bible_model = word2vec.Word2Vec(discard_punctuation_and_lowercased_sents, min_count=5, size=300)\n",
    "\n",
    "print('bible done')\n",
    "wiki = WikiCorpus('data/sw/swwiki-latest-pages-articles.xml.bz2', \n",
    "                  lemmatize=False, dictionary={})\n",
    "sentences = list(wiki.get_texts())\n",
    "sents = sentences + discard_punctuation_and_lowercased_sents\n",
    "print('len')\n",
    "print(len(sents))\n",
    "params = {'size': 300, 'window': 10, 'min_count': 10, \n",
    "          'workers': max(1, multiprocessing.cpu_count() - 1), 'sample': 1E-3,}\n",
    "wiki_model = Word2Vec(sents, **params).wv\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
